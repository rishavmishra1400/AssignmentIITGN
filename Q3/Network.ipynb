{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Network.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VPlzfIUpHkhu"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def squish(z):\n",
        "    a = 1/(1+np.exp(-z))\n",
        "    return a\n",
        "\n",
        "def squishprime(z):\n",
        "    fprime = np.multiply(squish(z),(1-squish(z)))\n",
        "    return fprime\n",
        "\n",
        "class NeuralNetwork():\n",
        "    #lis contains sizes of each layer\n",
        "    def __init__(self, lis): \n",
        "        \n",
        "        self.n = len(lis) # number of layers in total\n",
        "        self.reset()\n",
        "        self.WML = [[[0]] ] #dummy weight matrix associated with first layer\n",
        "        self.BL = [[[0]] ] #dummy bias matrix/list associated with first layer\n",
        "        self.Jprogress = []\n",
        "        \n",
        "        #initialize random weight matrices here...\n",
        "        for i in range(len(lis)-1):\n",
        "            #random weight matrix per layer\n",
        "            shape = (lis[i], lis[i+1])\n",
        "            wm = np.random.randn(lis[i], lis[i+1])\n",
        "            self.WML.append(wm)\n",
        "            #random biases per layer\n",
        "            b = np.random.randn(1, lis[i+1])\n",
        "            self.BL.append(b)\n",
        "            \n",
        "    def reset(self):\n",
        "        \n",
        "        self.AL = [] #list of layer activations\n",
        "        self.FprimeL = [[] ] #AL0 does not have z value to find fprime\n",
        "        self.DL = [[[]] ] #AL0 does not have delta values       \n",
        "        self.err =[[]]\n",
        "        self.C =[]\n",
        "        self.J = 0\n",
        "    \n",
        "    def forward(self, X):\n",
        "        \n",
        "        self.reset()\n",
        "        self.AL.append(X)\n",
        "        for i in range(self.n-1):\n",
        "            wm = self.WML[i+1]\n",
        "            b = self.BL[i+1]\n",
        "            z = np.matmul(self.AL[i], wm) + b\n",
        "            a = squish(z)\n",
        "            fp = squishprime(z)\n",
        "            self.AL.append(a)\n",
        "            self.FprimeL.append(fp)\n",
        "        return self.AL[-1]\n",
        "        \n",
        "    def backward(self, Y):\n",
        "        \n",
        "        for i in range(1, self.n):\n",
        "            #i = 1 represents the last layer L\n",
        "            #then you count backwards\n",
        "            #E = dC/da\n",
        "            if(i == 1):\n",
        "                E = self.AL[-1] - Y\n",
        "            else:\n",
        "                wm = self.WML[-i + 1]  #weights of l+1 layer\n",
        "                E = np.matmul(self.DL[1], wm.T) #propogated errors\n",
        "            f = self.FprimeL[-i]\n",
        "            d = np.multiply(f, E)\n",
        "            self.DL.insert(1, d)\n",
        "\n",
        "    def updateWeights(self, alpha=0.2):\n",
        "        \n",
        "        for i in range(1, self.n):\n",
        "            al1 = self.AL[i-1]\n",
        "            dl2 = self.DL[i]\n",
        "            T = len(dl2) #number of training samples in batch\n",
        "            changeInWeights = np.matmul(al1.T, dl2)/T\n",
        "            changeInBiases = np.matmul(np.ones(len(dl2)).reshape(1,len(dl2)), dl2)/T\n",
        "            \n",
        "            self.WML[i] = self.WML[i] - alpha*changeInWeights\n",
        "            self.BL[i] = self.BL[i] - alpha*changeInBiases         \n",
        "        \n",
        "    def batchTrain(self, X, Y, alpha):\n",
        "        self.forward(X)\n",
        "        self.backward(Y)\n",
        "        self.updateWeights(alpha)\n",
        "        \n",
        "    def plotProgress(self):\n",
        "        plt.plot(self.Jprogress, 'ro', ms=2)\n",
        "        plt.xlabel('Epoch')\n",
        "        plt.ylabel('Cost')\n",
        "        plt.title('Cost progress with each epoch')\n",
        "        plt.show()\n",
        "\n",
        "    def train(self, X, Y, batchSize=10, epochs=1, alpha=0.2):\n",
        "        \n",
        "        for epoch in range(epochs):\n",
        "            for i in range(0, len(X), batchSize):\n",
        "                if(i+batchSize<len(X)):\n",
        "                    Xbatch = X[i:i+batchSize]\n",
        "                    Ybatch = Y[i:i+batchSize]\n",
        "                else:\n",
        "                    Xbatch = X[i:]\n",
        "                    Ybatch = Y[i:]\n",
        "                    \n",
        "                self.batchTrain(Xbatch, Ybatch, alpha)\n",
        "                \n",
        "            self.Jprogress.append(self.cost(X, Y))\n",
        "\n",
        "    def cost(self, X, Y):\n",
        "            err = self.forward(X) - Y\n",
        "            err2 = err**2\n",
        "            C = [sum(i) for i in err2/2] # C[t] gives cost of a sample\n",
        "            J = sum(C)#/len(X) #total cost after running a batch\n",
        "            return J\n",
        "\n",
        "    def classify(self, X):\n",
        "        return (self.forward(X)>0.5).astype('int32')\n"
      ]
    }
  ]
}